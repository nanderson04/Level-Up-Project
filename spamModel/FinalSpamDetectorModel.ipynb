{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2152a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "import re\n",
    "import string\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a7402374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initial setup steps and parameters for easy changeability\n",
    "\n",
    "SEQUENCE_LENGTH = 100\n",
    "TRAIN_TEST_SPLIT = 0.25\n",
    "MAX_FEATURES = 10000\n",
    "EMBEDDING_DIM = 16\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "298745a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load in the data from the file\n",
    "\n",
    "def load_data():\n",
    "    texts, labels = [], []\n",
    "    with open(\"data/SMSSpamCollection\") as f:\n",
    "        for line in f:\n",
    "            split = line.split()\n",
    "            labels.append(split[0].strip())\n",
    "            texts.append(' '.join(split[1:]).strip())\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "6a3b6174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data, x has the text, y has whether that text is spam or not (ham); \n",
    "    # both in arrays, use corresponding indices\n",
    "\n",
    "x, y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "69ae1dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the train, test, validation sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=TRAIN_TEST_SPLIT, random_state=412)\n",
    "X_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.1, random_state=102)\n",
    "\n",
    "train_size = len(X_train)\n",
    "val_size = len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d5b8c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the text and labels into one variable for each set\n",
    "\n",
    "# raw_train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "# raw_test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "# raw_val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "raw_train_x = np.array(X_train)\n",
    "raw_train_y = np.array(y_train)\n",
    "raw_test_x = np.array(X_test)\n",
    "raw_test_y = np.array(y_test)\n",
    "raw_val_x = np.array(X_val)\n",
    "raw_val_y = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "fd03af52",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras' has no attribute 'saving'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [207], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# standardizing text inputs\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaving\u001b[49m\u001b[38;5;241m.\u001b[39mregister_keras_serializable(package\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_standardization\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_standardization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcustom_standardization\u001b[39m(input_data):\n\u001b[1;32m      5\u001b[0m     lowercase \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mstrings\u001b[38;5;241m.\u001b[39mlower(input_data)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras' has no attribute 'saving'"
     ]
    }
   ],
   "source": [
    "# standardizing text inputs\n",
    "\n",
    "tf.keras.saving.register_keras_serializable(package=\"custom_standardization\", name=\"custom_standardization\")\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "5a5d8f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer used to tokenize the text\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    max_tokens=MAX_FEATURES,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=SEQUENCE_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "56c1b0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the vectorize_layer\n",
    "\n",
    "train_text = raw_train_dataset.map(lambda x,y : x)\n",
    "#vectorize_layer.adapt(train_text)\n",
    "vectorize_layer.adapt(raw_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "b5ada15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to vectorize all the text of all sets\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "def vectorize_text_only(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    vlayer = vectorize_layer(text)\n",
    "    vlayer = tf.reshape(vlayer, (100))\n",
    "    return vlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "deb4010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing all sets\n",
    "\n",
    "# train_dataset = raw_train_dataset.map(vectorize_text)\n",
    "# test_dataset = raw_test_dataset.map(vectorize_text)\n",
    "# val_dataset = raw_val_dataset.map(vectorize_text)\n",
    "\n",
    "# print(train_dataset)\n",
    "\n",
    "train_x_array = []\n",
    "for element in raw_train_x:\n",
    "    train_x_array.append(vectorize_text_only(element))\n",
    "    \n",
    "train_x = np.array(train_x_array)\n",
    "\n",
    "test_x_array = []\n",
    "for element in raw_test_x:\n",
    "    test_x_array.append(vectorize_text_only(element))\n",
    "    \n",
    "test_x = np.array(test_x_array)\n",
    "\n",
    "val_x_array = []\n",
    "for element in raw_val_x:\n",
    "    val_x_array.append(vectorize_text_only(element))\n",
    "    \n",
    "val_x = np.array(val_x_array)\n",
    "\n",
    "train_y_array = []\n",
    "for element in raw_train_y:\n",
    "    if element == \"ham\":\n",
    "        train_y_array.append(0)\n",
    "    else:\n",
    "        train_y_array.append(1)\n",
    "\n",
    "train_y = np.array(train_y_array)\n",
    "\n",
    "test_y_array = []\n",
    "for element in raw_test_y:\n",
    "    if element == \"ham\":\n",
    "        test_y_array.append(0)\n",
    "    else:\n",
    "        test_y_array.append(1)\n",
    "\n",
    "test_y = np.array(test_y_array)\n",
    "\n",
    "val_y_array = []\n",
    "for element in raw_val_y:\n",
    "    if element == \"ham\":\n",
    "        val_y_array.append(0)\n",
    "    else:\n",
    "        val_y_array.append(1)\n",
    "\n",
    "val_y = np.array(val_y_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9ab59d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5016, 100)\n",
      "(5016,)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "8a35bac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(MAX_FEATURES, EMBEDDING_DIM),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "779e7442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the loss function\n",
    "\n",
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "46c33ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 1s 2ms/step - loss: 0.5440 - binary_accuracy: 0.8513\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3827 - binary_accuracy: 0.8642\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3535 - binary_accuracy: 0.8642\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3388 - binary_accuracy: 0.8642\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3241 - binary_accuracy: 0.8642\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.3063 - binary_accuracy: 0.8642\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2862 - binary_accuracy: 0.8642\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2605 - binary_accuracy: 0.8660\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2335 - binary_accuracy: 0.8812\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 0.2087 - binary_accuracy: 0.9077\n"
     ]
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "spamDetect = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c62ba09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 847us/step - loss: 0.1919 - binary_accuracy: 0.9154\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19192317128181458, 0.9153515100479126]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating the model\n",
    "\n",
    "model.evaluate(test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "570e5f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting the model to work with strings\n",
    "\n",
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4d60f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4230df2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 905us/step - loss: 0.1918 - accuracy: 0.9167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.19184905290603638, 0.9166666865348816]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "export_model.evaluate(raw_train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bc4a6a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, _update_step_xla while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: SPAM_DETECTOR/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: SPAM_DETECTOR/assets\n"
     ]
    }
   ],
   "source": [
    "export_model.save('SPAM_DETECTOR', save_format=\"keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78af302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
